{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"cnn_CIFAR","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNhrFaW6gC8GjzQRzCxnGG5"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Tr45ZSuZjd7","executionInfo":{"status":"ok","timestamp":1614567275201,"user_tz":480,"elapsed":200442,"user":{"displayName":"Vivian Cheng","photoUrl":"","userId":"10374517085012709020"}},"outputId":"c690d275-ed7d-4b35-8019-173d6669edbd"},"source":["\n","import numpy as np\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Sequential (\n","            # 32-3+1 = 30*30*32\n","            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","            # 15*15*32\n","            nn.MaxPool2d(2,2)\n","        ) \n","\n","        self.conv2 = nn.Sequential (\n","            # 15-3+1 = 13*13*64\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        self.conv3 = nn.Sequential (\n","            # 13-4+1=10*10*128\n","            nn.Conv2d(in_channels=64,out_channels=128, kernel_size=4),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.05),\n","            # 5*5*128 \n","            nn.MaxPool2d(2,2)\n","        )\n","\n","        self.fc = nn.Sequential(\n","            # Fully connected: 3200->1024\n","            nn.Linear(3200, 1024),\n","            nn.ReLU(inplace=True),\n","            # Fully connected: 1024->512\n","            nn.Linear(1024,512),\n","            nn.ReLU(inplace=True),\n","            # Fully connected: 512->128\n","            nn.Linear(512,128),\n","            nn.Dropout(p=0.05),\n","            # Fully connected: 128->10\n","            nn.Linear(128,10)\n","        )\n","\n","    # CNN Forward pass\n","    def forward(self, x):\n","        x = x.cuda()\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = x.view(x.shape[0],-1)\n","        x = self.fc(x)\n","        return x\n","\n","PATH = './p2_model.pkl'\n","\n","def main():\n","    # Hyperparameters \n","    batch_size, num_epoch, lr = 64, 15, 0.001\n","\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    print(device)\n","    # load and transform dataset\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                            download=True, transform=transform)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                            shuffle=True, num_workers=2)\n","\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                        download=True, transform=transform)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                            shuffle=False, num_workers=2)\n","\n","    classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    net = CNN()\n","    net.to(device)\n","   \n","    # Cross Entropy Loss\n","    criterion = nn.CrossEntropyLoss().to(device)\n","    # Adam Optimizer\n","    optimizer = optim.Adam(net.parameters(), lr=lr)\n","    # Reduce LR on plateau\n","    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","\n","    for epoch in range(num_epoch):  # loop over the dataset multiple times\n","        for param_group in optimizer.param_groups:\n","            print(\"Epoch {}, learning rate: {}\".format(epoch, param_group['lr']))\n","        scheduler.step(epoch)\n","\n","        running_loss = 0.0\n","        \n","        for i, data in enumerate(trainloader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 500 == 499:    # print every 2000 mini-batches\n","                print('[%d, %5d] loss: %.3f' %\n","                    (epoch + 1, i + 1, running_loss / 2000))\n","                running_loss = 0.0\n","\n","    print('Finished Training')\n","\n","    torch.save(net.state_dict(), PATH)\n","\n","\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on the 10000 test images: %d %%' % (\n","        100 * correct / total))\n","\n","           \n","if __name__ == \"__main__\":\n","    main()\n","\n","\n"],"execution_count":76,"outputs":[{"output_type":"stream","text":["cuda\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 0, learning rate: 0.001\n","[1,   500] loss: 0.349\n","Epoch 1, learning rate: 0.001\n","[2,   500] loss: 0.231\n","Epoch 2, learning rate: 0.001\n","[3,   500] loss: 0.186\n","Epoch 3, learning rate: 0.001\n","[4,   500] loss: 0.154\n","Epoch 4, learning rate: 0.001\n","[5,   500] loss: 0.129\n","Epoch 5, learning rate: 0.001\n","[6,   500] loss: 0.112\n","Epoch 6, learning rate: 0.001\n","[7,   500] loss: 0.092\n","Epoch 7, learning rate: 0.001\n","[8,   500] loss: 0.078\n","Epoch 8, learning rate: 0.001\n","[9,   500] loss: 0.064\n","Epoch 9, learning rate: 0.001\n","[10,   500] loss: 0.052\n","Epoch 10, learning rate: 0.001\n","[11,   500] loss: 0.043\n","Epoch 11, learning rate: 0.001\n","[12,   500] loss: 0.019\n","Epoch 12, learning rate: 0.0001\n","[13,   500] loss: 0.009\n","Epoch 13, learning rate: 0.0001\n","[14,   500] loss: 0.006\n","Epoch 14, learning rate: 0.0001\n","[15,   500] loss: 0.004\n","Finished Training\n","Accuracy of the network on the 10000 test images: 80 %\n"],"name":"stdout"}]}]}